from fastapi import APIRouter, Query
from pydantic import BaseModel
from typing import List, Optional
import random

router = APIRouter()

# === AI Tutor Models ===
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    userId: int
    message: str
    topic: Optional[str] = "é€™å€‹æ¦‚å¿µ"
    mode: Optional[str] = "understanding"
    useSearch: Optional[bool] = False # New RAG toggle
    context: Optional[List[ChatMessage]] = []

class ChatResponse(BaseModel):
    response: str
    followUp: Optional[str] = None
    sources: Optional[List[str]] = None # New field for citations

class InterventionResponse(BaseModel):
    shouldIntervene: bool
    type: Optional[str] = None
    message: Optional[str] = None
    suggestion: Optional[str] = None
    action: Optional[dict] = None
    topic: Optional[str] = None
    subjectName: Optional[str] = None

# ... (Socratic prompts unchanged) ...

@router.post("/chat", response_model=ChatResponse)
async def chat_with_tutor(request: ChatRequest):
    """
    Socratic dialogue with AI tutor (Enhanced with RAG)
    """
    # 1. RAG Mode
    if request.useSearch:
        from app.services.rag import rag_service
        try:
            # Query vector DB
            docs = await rag_service.query(request.message)
            
            # Generate RAG response
            rag_response = await rag_service.get_socratic_response_with_context(
                request.message, 
                docs
            )
            
            # Extract distinct sources
            sources = list(set([d.metadata.get("source", "Unknown") for d in docs]))
            
            return ChatResponse(
                response=rag_response, 
                sources=sources,
                followUp="é‚„æœ‰å…¶ä»–é—œæ–¼é€™ä»½æ•™æçš„å•é¡Œå—ï¼Ÿ"
            )
        except Exception as e:
            print(f"RAG Error: {e}")
            return ChatResponse(
                response="æŠ±æ­‰ï¼Œæœå°‹ç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚æˆ‘å°‡åˆ‡æ›å›æ™®é€šè¼”å°æ¨¡å¼ã€‚",
                followUp=None
            )

    # 2. Standard Socratic Mode (Existing Logic)
    mode = request.mode or "understanding"
    topic = request.topic or "é€™å€‹æ¦‚å¿µ"
    
    # Get appropriate prompts for mode
    prompts = SOCRATIC_PROMPTS.get(mode, SOCRATIC_PROMPTS["understanding"])
    
    # Generate response based on context length (simulate deepening conversation)
    context_length = len(request.context)
    
    if context_length >= 6:
        # After 3 exchanges, provide summary
        response = f"""ğŸ¯ ç¶“éæˆ‘å€‘çš„å°è©±ï¼Œä½ å°ã€Œ{topic}ã€çš„ç†è§£å·²ç¶“æ›´æ·±å…¥äº†ï¼

ç¸½çµä¸€ä¸‹ä½ çš„é—œéµè¦‹è§£ï¼š
â€¢ ä½ èƒ½ç”¨è‡ªå·±çš„è©±è§£é‡‹æ ¸å¿ƒæ¦‚å¿µ
â€¢ ä½ èƒ½èˆ‰å‡ºå¯¦éš›æ‡‰ç”¨çš„ä¾‹å­
â€¢ ä½ æ³¨æ„åˆ°äº†å®¹æ˜“æ··æ·†çš„åœ°æ–¹

ğŸ’ª ç¹¼çºŒä¿æŒé€™ç¨®ä¸»å‹•æ€è€ƒçš„ç¿’æ…£ï¼"""
        followUp = None
    else:
        # Select random follow-up question
        encouragement = random.choice(ENCOURAGEMENT_MESSAGES)
        question = random.choice(prompts).replace("{topic}", topic)
        response = f"{encouragement}\n\n{question}"
        followUp = question
    
    return ChatResponse(response=response, followUp=followUp)


@router.get("/intervention", response_model=InterventionResponse)
async def check_intervention(userId: int = Query(1)):
    """
    Check if AI should proactively intervene
    """
    # In production, this would check:
    # - Time spent on current topic
    # - Error patterns in flashcards
    # - Session duration
    # - User's historical performance
    
    # For demo, randomly suggest interventions
    if random.random() < 0.3:  # 30% chance
        interventions = [
            {
                "type": "struggle",
                "message": "æˆ‘æ³¨æ„åˆ°ä½ åœ¨ã€Œæ¥µé™ã€é€™å€‹æ¦‚å¿µä¸ŠèŠ±äº†æ¯”è¼ƒå¤šæ™‚é–“",
                "suggestion": "è¦ä¸è¦è©¦è©¦ç”¨ Feynman æ–¹æ³•ä¾†è§£é‡‹å®ƒï¼Ÿ",
                "action": {"label": "é–‹å§‹å°è©±", "type": "chat"},
                "topic": "æ¥µé™",
                "subjectName": "å¾®ç©åˆ†"
            },
            {
                "type": "break",
                "message": "ä½ å·²ç¶“å°ˆæ³¨å­¸ç¿’ 45 åˆ†é˜äº†ï¼Œåšå¾—å¾ˆå¥½ï¼",
                "suggestion": "å»ºè­°ä¼‘æ¯ 5-10 åˆ†é˜ï¼Œè®“å¤§è…¦éå›ºè¨˜æ†¶",
                "action": {"label": "é–‹å§‹ä¼‘æ¯", "type": "break"}
            },
            {
                "type": "strategy",
                "message": "æ ¹æ“šä½ çš„å­¸ç¿’æ•¸æ“šï¼Œè©¦è©¦äº¤éŒ¯å­¸ç¿’å¯èƒ½æœƒæ›´æœ‰æ•ˆ",
                "suggestion": "åœ¨å¾®ç©åˆ†å’Œç·šæ€§ä»£æ•¸ä¹‹é–“åˆ‡æ›ï¼Œæå‡è¾¨åˆ¥èƒ½åŠ›",
                "action": {"label": "åˆ‡æ›ç§‘ç›®", "type": "switch"},
                "topic": "äº¤éŒ¯å­¸ç¿’",
                "subjectName": "å­¸ç¿’ç­–ç•¥"
            }
        ]
        
        intervention = random.choice(interventions)
        return InterventionResponse(
            shouldIntervene=True,
            **intervention
        )
    
    return InterventionResponse(shouldIntervene=False)


@router.get("/memory/{userId}")
async def get_user_memory(userId: int):
    """
    Get AI's memory of user preferences and patterns
    """
    # In production, this would retrieve from database
    return {
        "userId": userId,
        "weakConcepts": ["æ¥µé™", "Taylorå±•é–‹"],
        "strongConcepts": ["å°æ•¸åŸºæœ¬é‹ç®—", "ç©åˆ†æŠ€å·§"],
        "preferredStrategies": ["Pomodoro", "ä¸»å‹•å›æ†¶"],
        "bestStudyTimes": ["20:00-22:00"],
        "averageFocusScore": 75,
        "commonMistakes": [
            {"concept": "æ¥µé™", "pattern": "å¿½ç•¥å·¦æ¥µé™å³æ¥µé™çš„å€åˆ¥"},
            {"concept": "ç©åˆ†", "pattern": "å¿˜è¨˜åŠ å¸¸æ•¸C"}
        ],
        "recentProgress": [
            {"concept": "å°æ•¸", "improvement": "+15%"},
            {"concept": "é€£é–å¾‹", "improvement": "+8%"}
        ]
    }


@router.post("/memory/{userId}/update")
async def update_user_memory(userId: int, update: dict):
    """
    Update AI's memory with new observations
    """
    # In production, this would persist to database
    return {
        "success": True,
        "message": "Memory updated",
        "userId": userId
    }
